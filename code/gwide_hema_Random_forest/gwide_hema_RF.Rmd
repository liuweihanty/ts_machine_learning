---
title: "genome wide data random forest fitting"
author: "Weihan Liu"
date: "29/12/2019"
output: html_document
---

First, take a look of our testing and training data
```{r}
summary(hema_test)
str(hema_test)

summary(hema_train)
str(hema_train)
```

Load required packages
```{r}
library(rsample)      # data splitting 
library(randomForest) # basic implementation
library(ranger)       # a faster implementation of randomForest
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)          # an extremely fast java-based platform
```


Implement random forest
```{r}
set.seed(123)

# default RF model
m1 <- randomForest(
  formula = rank ~ .,
  data = hema_train)

m1

#plotting the error rate, the error rate stablizes after ~ 100 trees
plot(m1)
m1$mse
```


The plotted error rate above is based on the OOB sample error and can be accessed directly at m1$mse. Thus, we can find which number of trees providing the lowest error rate
```{r}
# number of trees with lowest MSE
which.min(m1$mse)
# RMSE of this optimal random forest
sqrt(m1$mse[which.min(m1$mse)])
```


Tuning
```{r}
# names of features
features <- setdiff(names(hema_train), "rank")

set.seed(123)

m2 <- tuneRF(x = hema_train[features],
             y = hema_train$rank,
             ntreeTry = 500,
             mtryStart = 3,
             stepFactor = 1,
             improve = 0.01,
             trace = FALSE)
m2
```


Use only the threshold columns to train a model
```{r}

```





